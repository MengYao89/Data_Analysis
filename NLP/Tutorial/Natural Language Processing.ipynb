{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizing Words and Sentences with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> NLTK is a leading platform for building Python programs to work with human language data.\n",
      "1 --> It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(example_text)\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print(index, '-->', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK\n",
      "is\n",
      "a\n",
      "leading\n",
      "platform\n",
      "for\n",
      "building\n",
      "Python\n",
      "programs\n",
      "to\n",
      "work\n",
      "with\n",
      "human\n",
      "language\n",
      "data\n",
      ".\n",
      "It\n",
      "provides\n",
      "easy-to-use\n",
      "interfaces\n",
      "to\n",
      "over\n",
      "50\n",
      "corpora\n",
      "and\n",
      "lexical\n",
      "resources\n",
      "such\n",
      "as\n",
      "WordNet\n",
      ",\n",
      "along\n",
      "with\n",
      "a\n",
      "suite\n",
      "of\n",
      "text\n",
      "processing\n",
      "libraries\n",
      "for\n",
      "classification\n",
      ",\n",
      "tokenization\n",
      ",\n",
      "stemming\n",
      ",\n",
      "tagging\n",
      ",\n",
      "parsing\n",
      ",\n",
      "and\n",
      "semantic\n",
      "reasoning\n",
      ",\n",
      "wrappers\n",
      "for\n",
      "industrial-strength\n",
      "NLP\n",
      "libraries\n",
      ",\n",
      "and\n",
      "an\n",
      "active\n",
      "discussion\n",
      "forum\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_text)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stop words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len:  66\n",
      "After filter len:  48\n",
      "new word: \n",
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial-strength', 'NLP', 'libraries', ',', 'active', 'discussion', 'forum', '.']\n",
      "filtered_word: \n",
      "['is', 'a', 'for', 'to', 'with', 'to', 'over', 'and', 'such', 'as', 'with', 'a', 'of', 'for', 'and', 'for', 'and', 'an']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_text)\n",
    "print('Original len: ', len(words))\n",
    "\n",
    "new_word = [word for word in words if word not in stop_words]\n",
    "filtered_word = [word for word in words if word in stop_words]\n",
    "print('After filter len: ', len(new_word))\n",
    "\n",
    "print('new word: ')\n",
    "print(new_word)\n",
    "\n",
    "print('filtered_word: ')\n",
    "print(filtered_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stemming words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "sbs = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk\n",
      "is\n",
      "a\n",
      "lead\n",
      "platform\n",
      "for\n",
      "build\n",
      "python\n",
      "program\n",
      "to\n",
      "work\n",
      "with\n",
      "human\n",
      "languag\n",
      "data\n",
      ".\n",
      "It\n",
      "provid\n",
      "easy-to-us\n",
      "interfac\n",
      "to\n",
      "over\n",
      "50\n",
      "corpora\n",
      "and\n",
      "lexic\n",
      "resourc\n",
      "such\n",
      "as\n",
      "wordnet\n",
      ",\n",
      "along\n",
      "with\n",
      "a\n",
      "suit\n",
      "of\n",
      "text\n",
      "process\n",
      "librari\n",
      "for\n",
      "classif\n",
      ",\n",
      "token\n",
      ",\n",
      "stem\n",
      ",\n",
      "tag\n",
      ",\n",
      "pars\n",
      ",\n",
      "and\n",
      "semant\n",
      "reason\n",
      ",\n",
      "wrapper\n",
      "for\n",
      "industrial-strength\n",
      "nlp\n",
      "librari\n",
      ",\n",
      "and\n",
      "an\n",
      "activ\n",
      "discuss\n",
      "forum\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read\n",
      "read\n",
      "shop\n",
      "shop\n",
      "quickli\n",
      "quick\n",
      "slice\n",
      "slice\n"
     ]
    }
   ],
   "source": [
    "stem_sample = ['reading','shopping', 'quickly', 'sliced']\n",
    "for word in stem_sample:\n",
    "    print(ps.stem(word))\n",
    "    print(sbs.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lemmatization\n",
    "\n",
    "> Lemmatization is a more methodical way of converting all the grammatical/in ected forms of the root of the word. Lemmatization uses context and part of speech to determine the in ected form of the word and applies different normalization rules for each part of speech to get the root word (lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wlem = WordNetLemmatizer()\n",
    "wlem.lemmatize(\"ate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not work?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Part of Speech Tagging with NLTK \n",
    "Alphabetical list of part-of-speech tags used in the Penn Treebank Project\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  NLTK <--> POS-tag:  NNP\n",
      "Word:  is <--> POS-tag:  VBZ\n",
      "Word:  a <--> POS-tag:  DT\n",
      "Word:  leading <--> POS-tag:  VBG\n",
      "Word:  platform <--> POS-tag:  NN\n",
      "Word:  for <--> POS-tag:  IN\n",
      "Word:  building <--> POS-tag:  VBG\n",
      "Word:  Python <--> POS-tag:  NNP\n",
      "Word:  programs <--> POS-tag:  NNS\n",
      "Word:  to <--> POS-tag:  TO\n",
      "Word:  work <--> POS-tag:  VB\n",
      "Word:  with <--> POS-tag:  IN\n",
      "Word:  human <--> POS-tag:  JJ\n",
      "Word:  language <--> POS-tag:  NN\n",
      "Word:  data <--> POS-tag:  NNS\n",
      "Word:  . <--> POS-tag:  .\n",
      "Word:  It <--> POS-tag:  PRP\n",
      "Word:  provides <--> POS-tag:  VBZ\n",
      "Word:  easy-to-use <--> POS-tag:  JJ\n",
      "Word:  interfaces <--> POS-tag:  NNS\n",
      "Word:  to <--> POS-tag:  TO\n",
      "Word:  over <--> POS-tag:  IN\n",
      "Word:  50 <--> POS-tag:  CD\n",
      "Word:  corpora <--> POS-tag:  NNS\n",
      "Word:  and <--> POS-tag:  CC\n",
      "Word:  lexical <--> POS-tag:  JJ\n",
      "Word:  resources <--> POS-tag:  NNS\n",
      "Word:  such <--> POS-tag:  JJ\n",
      "Word:  as <--> POS-tag:  IN\n",
      "Word:  WordNet <--> POS-tag:  NNP\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  along <--> POS-tag:  IN\n",
      "Word:  with <--> POS-tag:  IN\n",
      "Word:  a <--> POS-tag:  DT\n",
      "Word:  suite <--> POS-tag:  NN\n",
      "Word:  of <--> POS-tag:  IN\n",
      "Word:  text <--> POS-tag:  NN\n",
      "Word:  processing <--> POS-tag:  NN\n",
      "Word:  libraries <--> POS-tag:  NNS\n",
      "Word:  for <--> POS-tag:  IN\n",
      "Word:  classification <--> POS-tag:  NN\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  tokenization <--> POS-tag:  NN\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  stemming <--> POS-tag:  VBG\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  tagging <--> POS-tag:  VBG\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  parsing <--> POS-tag:  NN\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  and <--> POS-tag:  CC\n",
      "Word:  semantic <--> POS-tag:  JJ\n",
      "Word:  reasoning <--> POS-tag:  NN\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  wrappers <--> POS-tag:  NNS\n",
      "Word:  for <--> POS-tag:  IN\n",
      "Word:  industrial-strength <--> POS-tag:  JJ\n",
      "Word:  NLP <--> POS-tag:  NNP\n",
      "Word:  libraries <--> POS-tag:  NNS\n",
      "Word:  , <--> POS-tag:  ,\n",
      "Word:  and <--> POS-tag:  CC\n",
      "Word:  an <--> POS-tag:  DT\n",
      "Word:  active <--> POS-tag:  JJ\n",
      "Word:  discussion <--> POS-tag:  NN\n",
      "Word:  forum <--> POS-tag:  NN\n",
      "Word:  . <--> POS-tag:  .\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(example_text)\n",
    "\n",
    "for word,pos in nltk.pos_tag(words):\n",
    "    print('Word: ', word,'<-->','POS-tag: ',pos)\n",
    "    \n",
    "# This is one of the pre-trained POS taggers that comes with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK\n",
      "platform\n",
      "Python\n",
      "language\n",
      "WordNet\n",
      "suite\n",
      "text\n",
      "processing\n",
      "classification\n",
      "tokenization\n",
      "parsing\n",
      "reasoning\n",
      "NLP\n",
      "discussion\n",
      "forum\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(word_tokenize(example_text))\n",
    "all_nouns = [word for word, pos in tagged if pos in ['NN', 'NNP']]\n",
    "for nouns in all_nouns:\n",
    "    print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly two ways to achieve any tagging task in NLTK:\n",
    "1. Using NLTK's or another lib's **pre-trained tagger**, and applying it on the test data.\n",
    "2. Building or Training a tagger to be used on test data.\n",
    "\n",
    "Typically, tagging problems like POS tagging are seen as sequence labeling problems or a classi cation problem where people have tried generative and discriminative models to predict the right tag for the given token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Named Entity Recognition (NER)\n",
    "Aside from POS, one of the most common labeling problems is  nding entities in the text. Typically NER constitutes name, location, and organizations. There are NER systems that tag more entities than just three of these. The problem can be seen as\n",
    "a sequence, labeling the Named entities using the context and other features.  \n",
    "\n",
    "There are two ways of tagging the NER using NLTK. One is by using the pre-trained NER model that just scores the test data, the other is to build a Machine learning based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  is/VBZ\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  located/VBN\n",
      "  in/IN\n",
      "  (ORGANIZATION USA/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "example_sent = Sent = \"Mark is studying at Stanford University in California which is located in USA\"\n",
    "named = ne_chunk(nltk.pos_tag(nltk.word_tokenize(example_sent)), binary=False)\n",
    "# named.draw()\n",
    "print(named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
